{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "SemanticSearch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhU7By0H2okj"
      },
      "source": [
        "# The Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7NPyD3R2uJK",
        "outputId": "4632e3b6-4401-411b-fc16-e7be87d6f878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_u0qiAa2okn"
      },
      "source": [
        "Based on a user query, return an existing question which most closely resembles the user's query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "1JcAJbLZ2okp"
      },
      "source": [
        "# import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "# import tensorflow_hub as hub\n",
        "from nltk import RegexpTokenizer\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "import spacy\n",
        "EN = spacy.load('en_core_web_sm')\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "import logging\n",
        "logging.getLogger('tensorflow').disabled = True #OPTIONAL - to disable outputs from Tensorflow"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9T8PG0b2okw"
      },
      "source": [
        "## Import the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "brMSwD9n2okx",
        "outputId": "8c66f772-0872-4dbb-b10c-dfee47cda833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        }
      },
      "source": [
        "data = pd.read_csv('/content/gdrive/My Drive/Stackoverflow_VS_extension/Preprocessed_data.csv')\n",
        "data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_title</th>\n",
              "      <th>post_corpus</th>\n",
              "      <th>question_content</th>\n",
              "      <th>question_url</th>\n",
              "      <th>tags</th>\n",
              "      <th>overall_scores</th>\n",
              "      <th>answers_content</th>\n",
              "      <th>sentiment_polarity</th>\n",
              "      <th>sentiment_subjectivity</th>\n",
              "      <th>processed_title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Using 'in' to match an attribute of Python obj...</td>\n",
              "      <td>using match attribute python objects array nt ...</td>\n",
              "      <td>using match attribute python objects array nt ...</td>\n",
              "      <td>https://stackoverflow.com/questions/683</td>\n",
              "      <td>python|arrays|iteration</td>\n",
              "      <td>0.011301</td>\n",
              "      <td>Using a list comprehension would build a tempo...</td>\n",
              "      <td>0.163567</td>\n",
              "      <td>0.568209</td>\n",
              "      <td>using match attribute python objects array</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Python version of PHP's stripslashes</td>\n",
              "      <td>python version php stripslashes wrote piece co...</td>\n",
              "      <td>python version php stripslashes wrote piece co...</td>\n",
              "      <td>https://stackoverflow.com/questions/13454</td>\n",
              "      <td>python|string|escaping</td>\n",
              "      <td>0.001115</td>\n",
              "      <td>Python has a built-in escape() function analog...</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.519274</td>\n",
              "      <td>python version php stripslashes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Unicode vs UTF-8 confusion in Python / Django?</td>\n",
              "      <td>unicode vs utf8 confusion python django stumbl...</td>\n",
              "      <td>unicode vs utf8 confusion python django stumbl...</td>\n",
              "      <td>https://stackoverflow.com/questions/22149</td>\n",
              "      <td>python|django|unicode</td>\n",
              "      <td>0.006997</td>\n",
              "      <td>From Wikipedia on UTF-8:</td>\n",
              "      <td>0.082857</td>\n",
              "      <td>0.403250</td>\n",
              "      <td>unicode vs utf8 confusion python django</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Using Django time/date widgets in custom form</td>\n",
              "      <td>using django time date widgets custom form use...</td>\n",
              "      <td>using django time date widgets custom form use...</td>\n",
              "      <td>https://stackoverflow.com/questions/38601</td>\n",
              "      <td>python|django</td>\n",
              "      <td>0.041431</td>\n",
              "      <td>Starting in Django 1.2 RC1, if you're using th...</td>\n",
              "      <td>0.302423</td>\n",
              "      <td>0.599938</td>\n",
              "      <td>using django time date widgets custom form</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Can parallel traversals be done in MATLAB just...</td>\n",
              "      <td>parallel traversals done matlab python using f...</td>\n",
              "      <td>parallel traversals done matlab python using f...</td>\n",
              "      <td>https://stackoverflow.com/questions/49307</td>\n",
              "      <td>python|arrays|matlab|for-loop</td>\n",
              "      <td>0.002837</td>\n",
              "      <td>should be for example:</td>\n",
              "      <td>0.358333</td>\n",
              "      <td>0.752381</td>\n",
              "      <td>parallel traversals done matlab python</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147054</th>\n",
              "      <td>How can I insert spaces between words given a ...</td>\n",
              "      <td>insert spaces words given list lists coded let...</td>\n",
              "      <td>insert spaces words given list lists coded let...</td>\n",
              "      <td>https://stackoverflow.com/questions/63758260</td>\n",
              "      <td>python|list|dictionary|spacing</td>\n",
              "      <td>-0.000607</td>\n",
              "      <td>Just append another whitespace in array:</td>\n",
              "      <td>0.243050</td>\n",
              "      <td>0.873782</td>\n",
              "      <td>insert spaces words given list lists coded let...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147055</th>\n",
              "      <td>Django creates another media folder inside med...</td>\n",
              "      <td>django creates another media folder inside med...</td>\n",
              "      <td>django creates another media folder inside med...</td>\n",
              "      <td>https://stackoverflow.com/questions/63758482</td>\n",
              "      <td>python|django|python-imaging-library</td>\n",
              "      <td>-0.000176</td>\n",
              "      <td>The  parameter [Django-doc] is relative to the...</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>django creates another media folder inside med...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147056</th>\n",
              "      <td>Options for deploying Flask app that continuou...</td>\n",
              "      <td>options deploying flask app continuously web s...</td>\n",
              "      <td>options deploying flask app continuously web s...</td>\n",
              "      <td>https://stackoverflow.com/questions/63758866</td>\n",
              "      <td>python|flask|heroku|web-scraping|web-applications</td>\n",
              "      <td>-0.000607</td>\n",
              "      <td>Have you tried using Cron? There is no cost no...</td>\n",
              "      <td>0.044898</td>\n",
              "      <td>0.545003</td>\n",
              "      <td>options deploying flask app continuously web s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147057</th>\n",
              "      <td>Delete \"nan\" in python list</td>\n",
              "      <td>delete nan python list new python simple quest...</td>\n",
              "      <td>delete nan python list new python simple quest...</td>\n",
              "      <td>https://stackoverflow.com/questions/63758902</td>\n",
              "      <td>python|list</td>\n",
              "      <td>-0.000750</td>\n",
              "      <td>You can ... get creative:my_list = ['experienc...</td>\n",
              "      <td>0.138095</td>\n",
              "      <td>0.636310</td>\n",
              "      <td>delete nan python list</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147058</th>\n",
              "      <td>Django : python manage.py runserver (multiple ...</td>\n",
              "      <td>django python managepy runserver multiple erro...</td>\n",
              "      <td>django python managepy runserver multiple erro...</td>\n",
              "      <td>https://stackoverflow.com/questions/63759220</td>\n",
              "      <td>python|python-3.x|django|ubuntu</td>\n",
              "      <td>-0.000750</td>\n",
              "      <td>The first thing you should do is to create a v...</td>\n",
              "      <td>0.148396</td>\n",
              "      <td>0.531907</td>\n",
              "      <td>django python managepy runserver multiple errors</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>147059 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           original_title  ...                                    processed_title\n",
              "0       Using 'in' to match an attribute of Python obj...  ...         using match attribute python objects array\n",
              "1                    Python version of PHP's stripslashes  ...                    python version php stripslashes\n",
              "2          Unicode vs UTF-8 confusion in Python / Django?  ...            unicode vs utf8 confusion python django\n",
              "3           Using Django time/date widgets in custom form  ...         using django time date widgets custom form\n",
              "4       Can parallel traversals be done in MATLAB just...  ...             parallel traversals done matlab python\n",
              "...                                                   ...  ...                                                ...\n",
              "147054  How can I insert spaces between words given a ...  ...  insert spaces words given list lists coded let...\n",
              "147055  Django creates another media folder inside med...  ...  django creates another media folder inside med...\n",
              "147056  Options for deploying Flask app that continuou...  ...  options deploying flask app continuously web s...\n",
              "147057                        Delete \"nan\" in python list  ...                             delete nan python list\n",
              "147058  Django : python manage.py runserver (multiple ...  ...   django python managepy runserver multiple errors\n",
              "\n",
              "[147059 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZaqdLtQ2ok1"
      },
      "source": [
        "## Import saved WordEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "WwC92GVf2ok2"
      },
      "source": [
        "# Import saved Wordvec Embeddings\n",
        "import gensim\n",
        "w2v_model = gensim.models.word2vec.Word2Vec.load('/content/gdrive/My Drive/Stackoverflow_VS_extension/SO_word2vec_embeddings.bin')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlEeevzh2ok7"
      },
      "source": [
        "## Calculate Sentence Embeddings\n",
        "In order to calculate the embeddings for an entire sentence, I defined the following function which averages the the embeddings for each valid token "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "Q2-Tp-PB2ok8"
      },
      "source": [
        "def question_to_vec(question, embeddings, dim=300):\n",
        "    question_embedding = np.zeros(dim)\n",
        "    valid_words = 0\n",
        "    for word in question.split(' '):\n",
        "        if word in embeddings:\n",
        "            valid_words += 1\n",
        "            question_embedding += embeddings[word]\n",
        "    if valid_words > 0:\n",
        "        return question_embedding/valid_words\n",
        "    else:\n",
        "        return question_embedding"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "UoMqYufr2olA",
        "outputId": "11e03837-264c-46ea-a6cb-68f2c8eb8e3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "all_title_embeddings = []\n",
        "for title in data.processed_title:\n",
        "    all_title_embeddings.append(question_to_vec(title, w2v_model))\n",
        "all_title_embeddings = np.array(all_title_embeddings)\n",
        "\n",
        "embeddings = pd.DataFrame(data = all_title_embeddings)\n",
        "embeddings.to_csv('/content/gdrive/My Drive/Stackoverflow_VS_extension/title_embeddings.csv', index=False)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYn9sODj2olD"
      },
      "source": [
        "Since the number of titles have have is fixed, I saved the sentence embeddings for all titles in a .csv file to save computation time on future runs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "vwkLuBPv2olE"
      },
      "source": [
        "all_title_embeddings = pd.read_csv('/content/gdrive/My Drive/Stackoverflow_VS_extension/title_embeddings.csv').values"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AHrAH732olO"
      },
      "source": [
        "## Import the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "l3JjBNEu2olP"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "# Custom loss function to handle multilabel classification task\n",
        "def multitask_loss(y_true, y_pred):\n",
        "    # Avoid divide by 0\n",
        "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "    # Multi-task loss\n",
        "    return K.mean(K.sum(- y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred), axis=1))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqQrUq-h6Sq6",
        "outputId": "d8876f69-5172-41ac-fd62-907f761eaa40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# loading tokenizer\n",
        "import pickle\n",
        "with open('/content/gdrive/My Drive/Stackoverflow_VS_extension/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index)\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "W2V_SIZE = 300\n",
        "MAX_SEQUENCE_LENGTH = 300\n",
        "\n",
        "# Embedding matrix for the embedding layer\n",
        "embedding_matrix = np.zeros((vocab_size+1, W2V_SIZE))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 469467 unique tokens.\n",
            "(469468, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "HGo5x8s32olT",
        "outputId": "95c10f2e-55ae-4226-96f0-17f080877242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "# from keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout,Conv1D,GlobalMaxPool1D,GRU\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from datetime import datetime\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping,TensorBoard,ModelCheckpoint\n",
        "from time import time\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size+1, W2V_SIZE, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
        "model.add(GRU(300, activation='relu',kernel_initializer='he_normal'))\n",
        "model.add(Dense(400,activation='relu',kernel_initializer=\"he_normal\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(150,activation='relu'))\n",
        "model.add(Dense(500, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=multitask_loss,\n",
        "              optimizer=\"adam\")\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "import keras.losses\n",
        "\n",
        "model.load_weights('/content/gdrive/My Drive/Stackoverflow_VS_extension/Tag_predictor_weights.h5')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 300)          140840400 \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 300)               541800    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 400)               120400    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 400)               1600      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 150)               60150     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 500)               75500     \n",
            "=================================================================\n",
            "Total params: 141,639,850\n",
            "Trainable params: 798,650\n",
            "Non-trainable params: 140,841,200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "JscXAXlC2olY"
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "tag_encoder = MultiLabelBinarizer()\n",
        "def predict_tags(text, include_neutral=True):\n",
        "    # Tokenize text\n",
        "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    # Predict\n",
        "    prediction = model.predict([x_test])[0]\n",
        "    for i,value in enumerate(prediction):\n",
        "        if value > 0.5:\n",
        "            prediction[i] = 1\n",
        "        else:\n",
        "            prediction[i] = 0\n",
        "    tags = tag_encoder.inverse_transform(np.array([prediction]))\n",
        "    return tags"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "JpXufT572olh"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "# import inflect\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def tokenize_text(text):\n",
        "    \"Apply tokenization using spacy to docstrings.\"\n",
        "    tokens = EN.tokenizer(text)\n",
        "    return [token.text.lower() for token in tokens if not token.is_space]\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def normalize(words):\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = remove_stopwords(words)\n",
        "    return words\n",
        "\n",
        "def tokenize_code(text):\n",
        "    \"A very basic procedure for tokenizing code strings.\"\n",
        "    return RegexpTokenizer(r'\\w+').tokenize(text)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    return ' '.join(normalize(tokenize_text(text)))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WigwdHrN2oll"
      },
      "source": [
        "## Import the saved Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "bgZI-Jtg2oll"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 300\n",
        "import pickle\n",
        "with open('/content/gdrive/My Drive/Stackoverflow_VS_extension/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlfI6nXi2olq"
      },
      "source": [
        "## Getting the most similar results\n",
        "So the way we actually calculate the the most similar results, is by comparing how far each result is from the query in terms of distance. This can only be done if both the query and the results are in a shared vector space. **Fortunately, that is exactly what our word embeddings are for**. They create each sentence as a vector in the embedding space, which makes it easy for us to distinguish them. \n",
        "\n",
        "After we have those vectors, we can assign a **Similarity Measure** as a metric which measures the closeness of two vectors. Common examples are Cosine distance, Euclidean distance and more.\n",
        "\n",
        "**However, for this specific task, I decided to assign a custom similarity measure**. It is defined as follows:\n",
        "\n",
        "![Similarity Measure](jupyter_imgs/similaritymeasure.png)\n",
        "\n",
        "- It considers the cosine distance as a base measure\n",
        "- It takes into account the popularity of the post based on the votes it has received by users at StackOverflow\n",
        "- It takes into account the overall sentiment of the responses that people have made. A positive sentiment entails that the answers were helpful and thus is a good post "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "MTQmBtr92olr",
        "outputId": "bb2b556d-024d-4dbd-a71f-167ad26e98f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 913
        }
      },
      "source": [
        "from IPython.display import HTML\n",
        "import logging\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "search_string = \"Combine lists of lists\" \n",
        "search_string = ' '.join(normalize(tokenize_text(search_string)))\n",
        "results_returned = \"5\" \n",
        "search_vect = np.array([question_to_vec(search_string, w2v_model)])    # Vectorize the user query\n",
        "\n",
        "# Calculate Cosine similarites for the query and all titles\n",
        "cosine_similarities = pd.Series(cosine_similarity(search_vect, all_title_embeddings)[0])\n",
        "\n",
        "# Custom Similarity Measure\n",
        "cosine_similarities = cosine_similarities*(1 + 0.4*data.overall_scores + 0.1*(data.sentiment_polarity))\n",
        "\n",
        "output =\"\"\n",
        "for i,j in cosine_similarities.nlargest(int(results_returned)).iteritems():\n",
        "    output += '<a target=\"_blank\" href='+ str(data.question_url[i])+'><h2>' + data.original_title[i] + '</h2></a>'\n",
        "    output += '<h3> Similarity Score: ' + str(j) + '</h3>'\n",
        "    output += '<h3> Stackover Votes: ' + str(data.overall_scores[i]) + '</h3>'\n",
        "    output +='<p style=\"font-family:verdana; font-size:110%;\"> '\n",
        "    for i in data.question_content[i][:50].split():\n",
        "        if i.lower() in search_string:\n",
        "            output += \" <b>\"+str(i)+\"</b>\"\n",
        "        else:\n",
        "            output += \" \"+str(i)\n",
        "    output += \"</p><hr>\"\n",
        "    \n",
        "output = '<h3>Results:</h3>'+output\n",
        "display(HTML(output))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Results:</h3><a target=\"_blank\" href=https://stackoverflow.com/questions/39959106><h2>Mutability of Lists</h2></a><h3> Similarity Score: 0.912765455293682</h3><h3> Stackover Votes: -0.0003197625431142165</h3><p style=\"font-family:verdana; font-size:110%;\">  mutability <b>lists</b> <b>list</b> sorted perhaps ascending num</p><hr><a target=\"_blank\" href=https://stackoverflow.com/questions/44859136><h2>Combining lists of lists in python</h2></a><h3> Similarity Score: 0.9068700976341155</h3><h3> Stackover Votes: -3.281849720316921e-05</h3><p style=\"font-family:verdana; font-size:110%;\">  combining <b>lists</b> <b>lists</b> python want <b>combine</b> <b>list</b> <b>lis</b></p><hr><a target=\"_blank\" href=https://stackoverflow.com/questions/2407398><h2>How to merge lists into a list of tuples?</h2></a><h3> Similarity Score: 0.9030012426367231</h3><h3> Stackover Votes: 0.09064150001068777</h3><p style=\"font-family:verdana; font-size:110%;\">  merge <b>lists</b> <b>list</b> tuples pythonic approach achieve</p><hr><a target=\"_blank\" href=https://stackoverflow.com/questions/59644552><h2>How to remove all duplicate lists and lists that are subset of other lists from an array of lists?</h2></a><h3> Similarity Score: 0.9011693268303629</h3><h3> Stackover Votes: -0.0006067065890252638</h3><p style=\"font-family:verdana; font-size:110%;\">  remove duplicate <b>lists</b> <b>lists</b> subset <b>lists</b> array <b>li</b></p><hr><a target=\"_blank\" href=https://stackoverflow.com/questions/50180242><h2>Combine each element from two lists of lists python</h2></a><h3> Similarity Score: 0.9006882998640888</h3><h3> Stackover Votes: -0.0003197625431142165</h3><p style=\"font-family:verdana; font-size:110%;\">  <b>combine</b> element two <b>lists</b> <b>lists</b> python struggling</p><hr>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtmtVMZs6u-c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}